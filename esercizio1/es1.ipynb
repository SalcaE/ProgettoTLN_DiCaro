{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio Lab-1 Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as panda\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestione dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono state lette le definizioni nel file fornito e inserite in un dizionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(location):\n",
    "    data = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "\n",
    "    fileText= panda.read_csv(location, sep='\\t', header=None)\n",
    "    for row in fileText.columns[1:]:\n",
    "        x= fileText[row].tolist()\n",
    "        key=  x.pop(0)\n",
    "        data[key] = x\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo passo è stato quello di elaborare il testo rimuovendo le stopwords e ottenendo i vari tag per poi lemmatizzare le parole. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatizzation(data):\n",
    "    lemmas_set = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop = set(stopwords.words('english') + list(string.punctuation) + [\"'s\", \"'\", \"n't\",\"\\\"\", \"``\", \"'d\", \"'re\", \"''\",\"''\"])\n",
    "\n",
    "    for key, values in data.items():\n",
    "        tokens = [word_tokenize(i.lower()) for i in values]\n",
    "        for x in tokens:\n",
    "            lemmas = []\n",
    "            index = tokens.index(x) \n",
    "            tmp = [word for word in x if not word in stop]\n",
    "            tmp_line = list(map(lambda x: (x[0], pos_tagger(x[1])), nltk.pos_tag(tmp)))\n",
    "            for word, tag in tmp_line:\n",
    "                if tag is None:\n",
    "                    lemmas.append(word)\n",
    "                else: \n",
    "                    lemmas.append(lemmatizer.lemmatize(word, tag))\n",
    "            tokens[index] = lemmas #sovrascrivi nella stessa posizione la nuova frase lemmatizata\n",
    "        lemmas_set[key] = tokens \n",
    "    return lemmas_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il calcolo della similarity sono stati utilizzati due approcci, il primo prevede l'utilizzo della distanza euclidea mentre il secondo quello della cosine-similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(lemmas):\n",
    "    similarity = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "    for key, values in lemmas.items():\n",
    "        sum = 0\n",
    "        document=[]\n",
    "        vectorizer = CountVectorizer()\n",
    "        for elem in values:\n",
    "            document.append(\" \".join(elem)) #per ogni array convertilo in stringa e mettilo in document\n",
    "        vector = vectorizer.fit_transform(document)\n",
    "        \n",
    "        cont = 0\n",
    "        for x in vector.toarray().tolist():\n",
    "            i = vector.toarray().tolist().index(x)\n",
    "            \n",
    "            for y in vector.toarray().tolist():\n",
    "                j = vector.toarray().tolist().index(y)\n",
    "                \n",
    "                if i != j:\n",
    "\n",
    "                    #sum = sum + dist_euclidea(vector[0],vector[1])\n",
    "                    sum = sum + cosine_similarity(vector[i].toarray(),vector[j].toarray())[0][0]\n",
    "                    \n",
    "                    cont = cont + 1\n",
    "                       \n",
    "        similarity[key] = (sum/cont) \n",
    "    return similarity\n",
    "\n",
    "def dist_euclidea(v0,v1):\n",
    "    a= np.array(v0.toarray())\n",
    "    b= np.array(v1.toarray())\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I risultati ottenuti nella similarità mostrano che si hanno risultati migliori con concetti concreti rispetto agli astratti, in particolar modo usando la cosine similarity possiamo notare come sia più difficile elaborare un concetto specifico e astratto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Result               </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> Astratto </span>┃<span style=\"font-weight: bold\"> Concreto </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 0.136713 │ 0.153571 │\n",
       "│ Specifico │ 0.057165 │ 0.474761 │\n",
       "└───────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Result               \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAstratto\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcreto\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 0.136713 │ 0.153571 │\n",
       "│ Specifico │ 0.057165 │ 0.474761 │\n",
       "└───────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distanza euclidea\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Result               </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> Astratto </span>┃<span style=\"font-weight: bold\"> Concreto </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 2.236068 │ 3.0      │\n",
       "│ Specifico │ 2.44949  │ 3.605551 │\n",
       "└───────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Result               \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAstratto\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcreto\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 2.236068 │ 3.0      │\n",
       "│ Specifico │ 2.44949  │ 3.605551 │\n",
       "└───────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_table(results):\n",
    "    console = Console()\n",
    "    table = Table(title=\"Result\")\n",
    "    table.add_column(\"\")\n",
    "    table.add_column(\"Astratto\")\n",
    "    table.add_column(\"Concreto\")\n",
    "    table.add_row(\"Generico\", str(round(results['pain'], 6)), str(round(results['door'], 6)))\n",
    "    table.add_row(\"Specifico\", str(round(results['blurriness'], 6)), str(round(results['ladybug'], 6)))\n",
    "    console.print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
