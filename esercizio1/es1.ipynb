{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio Lab-1 Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as panda\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import wordnet\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "import string\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gestione dati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sono state lette le definizioni nel file fornito e inserite in un dizionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(location):\n",
    "    data = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "\n",
    "    fileText= panda.read_csv(location, sep='\\t', header=None)\n",
    "    for row in fileText.columns[1:]:\n",
    "        x= fileText[row].tolist()\n",
    "        key=  x.pop(0)\n",
    "        data[key] = x\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il primo passo è stato quello di elaborare il testo rimuovendo le stopwords e lemmatizzando le parole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:         \n",
    "        return None\n",
    "\n",
    "\n",
    "def lemmatizzation(data):\n",
    "    lemmas_set = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer() #classe contentente tutti i lemmi(parole base)\n",
    "    stop = set(stopwords.words('english') + list(string.punctuation) + [\"'s\", \"'\", \"n't\",\"\\\"\", \"``\", \"'d\", \"'re\", \"''\",\"''\"])#rimozioni di parole e punteggiature \n",
    "\n",
    "    for key, values in data.items(): #values contiene tutte le frasi già divise su key\n",
    "        tokens = [word_tokenize(i.lower()) for i in values]#trasformo ogni paraola delle frasi in token e le metto lower\n",
    "        for x in tokens:\n",
    "            lemmas = []\n",
    "            index = tokens.index(x) #prendo indice nell'array di token/frasi\n",
    "            tmp = [word for word in x if not word in stop] #in tmp ho la frase pulita con le stop\n",
    "            tmp_line = list(map(lambda x: (x[0], pos_tagger(x[1])), nltk.pos_tag(tmp))) #fai tagging VEDI\n",
    "            for word, tag in tmp_line:\n",
    "                if tag is None:\n",
    "                    lemmas.append(word) #copia diretto\n",
    "                else: #se il tag è conosciuto:      \n",
    "                    lemmas.append(lemmatizer.lemmatize(word, tag)) #metti parole base/lemmatizzata\n",
    "            tokens[index] = lemmas #sovrascrivi nella stessa posizione la nuova frase lemmatizata <3\n",
    "        lemmas_set[key] = tokens #metti all'interno della giusta key del dizionario l'insieme di parole lemmatizzate (edu chan <3)\n",
    "    return lemmas_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per il calcolo della similarity sono stati utilizzati due approcci, il primo prevede l'utilizzo della distanza euclidea mentre il secondo quello della cosine-similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(lemmas):\n",
    "    similarity = {\n",
    "        'door': [],\n",
    "        'ladybug': [],\n",
    "        'pain': [],\n",
    "        'blurriness': []\n",
    "    }\n",
    "    for key, values in lemmas.items():\n",
    "        sum = 0\n",
    "        document=[]\n",
    "        vectorizer = CountVectorizer()#per creare la matrice di parole uniche\n",
    "        for elem in values:#per ogni array token\n",
    "            document.append(\" \".join(elem)) #per ogni array convertilo in stringa e mettilo in document\n",
    "        vectorizer.fit(document)#magia\n",
    "        vector = vectorizer.transform(document) #codifica il documuento\n",
    "\n",
    "        #print(\"Vocabulary: \", vectorizer.vocabulary_)\n",
    "        \n",
    "        cont = 0 #reset per ogni key nuova\n",
    "        for x in vector.toarray().tolist():\n",
    "            i = vector.toarray().tolist().index(x)#prendo la posizione dell'array dentro vector (all'inizio il primo..)\n",
    "            \n",
    "            for y in vector.toarray().tolist():#prendo element\n",
    "                j = vector.toarray().tolist().index(y) #prendo posizione di y\n",
    "                \n",
    "                if i != j : #entra tranne quando è se stesso l'array \n",
    "\n",
    "                    #sum = sum + dist_euclidea(vector[0],vector[1])\n",
    "                    sum = sum + cosine_similarity(vector[i].toarray(),vector[j].toarray())[0][0]\n",
    "                    \n",
    "                    cont = cont + 1 #cont= Totrighe * TotRighe-1\n",
    "                       \n",
    "        similarity[key] = (sum/cont) #metto la media tra totale valore/volte che è stato calcolato\n",
    "    return similarity\n",
    "\n",
    "def dist_euclidea(v0,v1):\n",
    "    a= np.array(v0.toarray())\n",
    "    b= np.array(v1.toarray())\n",
    "    return np.linalg.norm(a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risultati"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I risultati ottenuti nella similarità mostrano che si hanno risultati migliori con concetti concreti rispetto agli astratti, in particolar modo usando la cosine similarity possiamo notare come sia più difficile elaborare un concetto specifico e astratto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Result               </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> Astratto </span>┃<span style=\"font-weight: bold\"> Concreto </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 0.136713 │ 0.153571 │\n",
       "│ Specifico │ 0.057165 │ 0.474761 │\n",
       "└───────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Result               \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAstratto\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcreto\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 0.136713 │ 0.153571 │\n",
       "│ Specifico │ 0.057165 │ 0.474761 │\n",
       "└───────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distanza euclidea\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">              Result               </span>\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">           </span>┃<span style=\"font-weight: bold\"> Astratto </span>┃<span style=\"font-weight: bold\"> Concreto </span>┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 2.236068 │ 3.0      │\n",
       "│ Specifico │ 2.44949  │ 3.605551 │\n",
       "└───────────┴──────────┴──────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m              Result               \u001b[0m\n",
       "┏━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mAstratto\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConcreto\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n",
       "│ Generico  │ 2.236068 │ 3.0      │\n",
       "│ Specifico │ 2.44949  │ 3.605551 │\n",
       "└───────────┴──────────┴──────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def print_table(results):\n",
    "    console = Console()\n",
    "    table = Table(title=\"Result\")\n",
    "    table.add_column(\"\")#prima colonna vuota\n",
    "    table.add_column(\"Astratto\")\n",
    "    table.add_column(\"Concreto\")\n",
    "    table.add_row(\"Generico\", str(round(results['pain'], 6)), str(round(results['door'], 6)))#round arrotonda le cifre decimali a 6\n",
    "    table.add_row(\"Specifico\", str(round(results['blurriness'], 6)), str(round(results['ladybug'], 6)))\n",
    "    console.print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
